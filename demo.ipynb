{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLACE: Proximity Learning of Articulation and Contact in 3D Environments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick demo to generate random bodies given a scene mesh and visualize the results. Three steps are included:\n",
    "* Use the pretrained C-VAE to random generate body meshes\n",
    "* Optimization stage 1: perform simple optimization (without interaction-based losses)\n",
    "* Optimization stage 2: perform advanced optimizatioin (interaction-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dependencies, set data/model paths, and set hype-parameters for optimization\n",
    "we use PROX dataset, scene 'N3OpenArea', and please set the smplx/vpose model paths according to your configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# from open3d import JVisualizer\n",
    "from open3d.j_visualizer import JVisualizer\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from human_body_prior.tools.model_loader import load_vposer\n",
    "import open3d as o3d\n",
    "import smplx\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import chamfer_pytorch.dist_chamfer as ext\n",
    "from models.cvae import *\n",
    "from preprocess.preprocess_optimize import *\n",
    "from preprocess.bps_encoding import *\n",
    "from utils import *\n",
    "from utils_read_data import *\n",
    "\n",
    "prox_dataset_path = '/mnt/hdd/PROX'\n",
    "scene_name = 'N3OpenArea'\n",
    "# smplx/vpose model path\n",
    "smplx_model_path = '/mnt/hdd/PROX/body_models/smplx_model'\n",
    "vposer_model_path = '/mnt/hdd/PROX/body_models/vposer_v1_0'\n",
    "\n",
    "# set optimization hype-parameters\n",
    "weight_loss_rec_verts = 1.0\n",
    "weight_loss_rec_bps = 3.0\n",
    "weight_loss_vposer = 0.02\n",
    "weight_loss_shape = 0.01\n",
    "weight_loss_hand = 0.01\n",
    "weight_collision = 8.0\n",
    "weight_loss_contact = 0.5\n",
    "itr_s1 = 200\n",
    "itr_s2 = 100\n",
    "\n",
    "cube_size = 2.0  # 3D cage size\n",
    "optimize = True  # optimize or not\n",
    "\n",
    "# trained model path\n",
    "scene_bps_AE_path = 'checkpoints/sceneBpsAE_last_model.pkl'\n",
    "cVAE_path = 'checkpoints/cVAE_last_model.pkl'\n",
    "scene_verts_AE_path = 'checkpoints/sceneBpsVertsAE_last_model.pkl'\n",
    "bodyDec_path = 'checkpoints/body_dec_last_model.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load scene mesh, scene SDF, smplx model, vposer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL-X model, with only 10 shape coefficients.\n",
      "WARNING: You are using a SMPL-X model, with only 10 shape and 10 expression coefficients.\n",
      "[INFO] smplx model loaded.\n",
      "Found Trained Model: /mnt/hdd/PROX/body_models/vposer_v1_0/snapshots/TR00_E096.pt\n",
      "[INFO] vposer model loaded\n"
     ]
    }
   ],
   "source": [
    "# read scen mesh/sdf\n",
    "scene_mesh, cur_scene_verts, s_grid_min_batch, s_grid_max_batch, s_sdf_batch = read_mesh_sdf(prox_dataset_path,\n",
    "                                                                                             'prox',\n",
    "                                                                                             scene_name)\n",
    "smplx_model = smplx.create(smplx_model_path, model_type='smplx',\n",
    "                           gender='neutral', ext='npz',\n",
    "                           num_pca_comps=12,\n",
    "                           create_global_orient=True,\n",
    "                           create_body_pose=True,\n",
    "                           create_betas=True,\n",
    "                           create_left_hand_pose=True,\n",
    "                           create_right_hand_pose=True,\n",
    "                           create_expression=True,\n",
    "                           create_jaw_pose=True,\n",
    "                           create_leye_pose=True,\n",
    "                           create_reye_pose=True,\n",
    "                           create_transl=True,\n",
    "                           batch_size=1\n",
    "                           ).to(device)\n",
    "print('[INFO] smplx model loaded.')\n",
    "\n",
    "vposer_model, _ = load_vposer(vposer_model_path, vp_model='snapshot')\n",
    "vposer_model = vposer_model.to(device)\n",
    "print('[INFO] vposer model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. random select an area in the scene, and compute bps encodings\n",
    "random place a 3D cage with cube size of 2 inside the 3D scene, compute the scene bps encoding, body bps encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] scene mesh cropped and shifted.\n",
      "[INFO] bps encoding computed.\n"
     ]
    }
   ],
   "source": [
    "rot_angle_1, scene_min_x, scene_max_x, scene_min_y, scene_max_y = define_scene_boundary('prox', scene_name)\n",
    "\n",
    "scene_verts = rotate_scene_smplx_predefine(cur_scene_verts, rot_angle=rot_angle_1)\n",
    "scene_verts_local, scene_verts_crop_local, shift = crop_scene_cube_smplx_predifine(\n",
    "    scene_verts, r=cube_size, with_wall_ceilling=True, random_seed=np.random.randint(10000),\n",
    "    scene_min_x=scene_min_x, scene_max_x=scene_max_x, scene_min_y=scene_min_y, scene_max_y=scene_max_y,\n",
    "    rotate=True)\n",
    "print('[INFO] scene mesh cropped and shifted.')\n",
    "\n",
    "\n",
    "scene_basis_set = bps_gen_ball_inside(n_bps=10000, random_seed=100)\n",
    "scene_verts_global, scene_verts_crop_global, rot_angle_2 = \\\n",
    "    augmentation_crop_scene_smplx(scene_verts_local / cube_size,\n",
    "                                  scene_verts_crop_local / cube_size,\n",
    "                                  np.random.randint(10000))\n",
    "scene_bps, selected_scene_verts_global, selected_ind = bps_encode_scene(scene_basis_set,\n",
    "                                                                        scene_verts_crop_global)  # [n_feat, n_bps]\n",
    "selected_scene_verts_local = scene_verts_crop_local[selected_ind]\n",
    "print('[INFO] bps encoding computed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. load trained checkpoints, and random generate a body inside the selected area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] a random body is generated.\n"
     ]
    }
   ],
   "source": [
    "############################# load trained model ###############################\n",
    "scene_bps = torch.from_numpy(scene_bps).float().unsqueeze(0).to(device)  # [1, 1, n_bps]\n",
    "scene_bps_verts = torch.from_numpy(selected_scene_verts_local.transpose(1, 0)).float().unsqueeze(0).to(device)  # [1, 3, 10000]\n",
    "\n",
    "scene_bps_AE = BPSRecMLP(n_bps=10000, n_bps_feat=1, hsize1=1024, hsize2=512).to(device)\n",
    "weights = torch.load(scene_bps_AE_path, map_location=lambda storage, loc: storage)\n",
    "scene_bps_AE.load_state_dict(weights)\n",
    "\n",
    "c_VAE = BPS_CVAE(n_bps=10000, n_bps_feat=1, hsize1=1024, hsize2=512, eps_d=32).to(device)\n",
    "weights = torch.load(cVAE_path, map_location=lambda storage, loc: storage)\n",
    "c_VAE.load_state_dict(weights)\n",
    "\n",
    "scene_AE = Verts_AE(n_bps=10000, hsize1=1024, hsize2=512).to(device)\n",
    "weights = torch.load(scene_verts_AE_path, map_location=lambda storage, loc: storage)\n",
    "scene_AE.load_state_dict(weights)\n",
    "\n",
    "body_dec = Body_Dec_shift(n_bps=10000, n_bps_feat=1, hsize1=1024, hsize2=512, n_body_verts=10475,\n",
    "                          body_param_dim=75, rec_goal='body_verts').to(device)\n",
    "weights = torch.load(bodyDec_path, map_location=lambda storage, loc: storage)\n",
    "body_dec.load_state_dict(weights)\n",
    "\n",
    "scene_bps_AE.eval()\n",
    "c_VAE.eval()\n",
    "scene_AE.eval()\n",
    "body_dec.eval()\n",
    "\n",
    "\n",
    "######################## random sample a body  ##########################\n",
    "scene_bps_verts = scene_bps_verts / cube_size\n",
    "\n",
    "_, scene_bps_feat = scene_bps_AE(scene_bps)\n",
    "_, scene_bps_verts_feat = scene_AE(scene_bps_verts)\n",
    "body_bps_sample = c_VAE.sample(1, scene_bps_feat)  # [1, 1, 10000]\n",
    "body_verts_sample, body_shift = body_dec(body_bps_sample,\n",
    "                                         scene_bps_verts_feat)  # [1, 3, 10475], unit ball scale, local coordinate\n",
    "body_shift = body_shift.repeat(1, 1, 10475).reshape([body_verts_sample.shape[0], 10475, 3])  # [bs, 10475, 3]\n",
    "body_verts_sample = body_verts_sample + body_shift.permute(0, 2, 1)  # [bs=1, 3, 10475]\n",
    "print('[INFO] a random body is generated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Optimization stage 1: perform simple optimization (without interaction-based losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] start optimization stage 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:34<00:00,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] optimization stage 1 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load contact parts\n",
    "contact_part = ['L_Leg', 'R_Leg']\n",
    "vid, _ = get_contact_id(body_segments_folder=os.path.join(prox_dataset_path, 'body_segments'),\n",
    "                        contact_body_parts=contact_part)\n",
    "        \n",
    "################ stage 1 (simple optimization, without contact/collision loss) ######\n",
    "print('[INFO] start optimization stage 1...')\n",
    "body_params_rec = torch.randn(1, 72).to(device)  # initialize smplx params, bs=1, local 3D cage coordinate system\n",
    "body_params_rec[0, 0] = 0.0\n",
    "body_params_rec[0, 1] = 0.0\n",
    "body_params_rec[0, 2] = 0.0\n",
    "body_params_rec[0, 3] = 1.5\n",
    "body_params_rec[0, 4] = 0.0\n",
    "body_params_rec[0, 5] = 0.0\n",
    "body_params_rec = convert_to_6D_rot(body_params_rec)\n",
    "body_params_rec.requires_grad = True\n",
    "optimizer = optim.Adam([body_params_rec], lr=0.1)\n",
    "\n",
    "body_verts = body_verts_sample.permute(0, 2, 1)  # [1, 10475, 3]\n",
    "body_verts = body_verts * cube_size  # to local 3d cage coordinate system scale\n",
    "\n",
    "for step in tqdm(range(itr_s1)):\n",
    "    if step > 100:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.01\n",
    "    if step > 300:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.001\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    body_params_rec_72 = convert_to_3D_rot(body_params_rec)  # tensor, [bs=1, 72]\n",
    "    body_pose_joint = vposer_model.decode(body_params_rec_72[:, 16:48], output_type='aa').view(1, -1)\n",
    "    body_verts_rec = gen_body_mesh(body_params_rec_72, body_pose_joint, smplx_model)[0]  # [n_body_vert, 3]\n",
    "\n",
    "    # transform body verts to unit ball global coordinate system\n",
    "    temp = body_verts_rec / cube_size  # scale into unit ball\n",
    "    body_verts_rec_global = torch.zeros(body_verts_rec.shape).to(device)\n",
    "    body_verts_rec_global[:, 0] = temp[:, 0] * math.cos(rot_angle_2) - temp[:, 1] * math.sin(rot_angle_2)\n",
    "    body_verts_rec_global[:, 1] = temp[:, 0] * math.sin(rot_angle_2) + temp[:, 1] * math.cos(rot_angle_2)\n",
    "    body_verts_rec_global[:, 2] = temp[:, 2]\n",
    "\n",
    "    # calculate optimized body bps feature\n",
    "    body_bps_rec = torch.zeros(body_bps_sample.shape)\n",
    "    if weight_loss_rec_bps > 0:\n",
    "        nbrs = NearestNeighbors(n_neighbors=1, leaf_size=1, algorithm=\"ball_tree\").fit(\n",
    "            body_verts_rec_global.detach().cpu().numpy())\n",
    "        neigh_dist, neigh_ind = nbrs.kneighbors(selected_scene_verts_global)\n",
    "        body_bps_rec = body_verts_rec_global[neigh_ind.squeeze()] - \\\n",
    "                        torch.from_numpy(selected_scene_verts_global).float().to(device)  # [n_bps, 3]\n",
    "        body_bps_rec = torch.sqrt(\n",
    "            body_bps_rec[:, 0] ** 2 + body_bps_rec[:, 1] ** 2 + body_bps_rec[:, 2] ** 2).unsqueeze(0).unsqueeze(0)  # [bs=1, 1, n_bps]\n",
    "\n",
    "    ### body bps feature reconstruct loss\n",
    "    loss_rec_verts = F.l1_loss(body_verts_rec.unsqueeze(0), body_verts)\n",
    "    loss_rec_bps = F.l1_loss(body_bps_sample, body_bps_rec)\n",
    "\n",
    "    ### vposer loss\n",
    "    body_params_rec_72 = convert_to_3D_rot(body_params_rec)\n",
    "    vposer_pose = body_params_rec_72[:, 16:48]\n",
    "    loss_vposer = torch.mean(vposer_pose ** 2)\n",
    "    ### shape prior loss\n",
    "    shape_params = body_params_rec_72[:, 6:16]\n",
    "    loss_shape = torch.mean(shape_params ** 2)\n",
    "    ### hand pose prior loss\n",
    "    hand_params = body_params_rec_72[:, 48:]\n",
    "    loss_hand = torch.mean(hand_params ** 2)\n",
    "\n",
    "    loss = weight_loss_rec_verts * loss_rec_verts + weight_loss_rec_bps * loss_rec_bps + \\\n",
    "           weight_loss_vposer * loss_vposer + \\\n",
    "           weight_loss_shape * loss_shape + \\\n",
    "           weight_loss_hand * loss_hand\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "print('[INFO] optimization stage 1 finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Optimization stage 2: perform advanced optimizatioin (interaction-based), with contact and collision loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] start optimization stage 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:18<00:00,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] optimization stage 2 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] start optimization stage 2...')\n",
    "optimizer = optim.Adam([body_params_rec], lr=0.01)\n",
    "\n",
    "body_verts = body_verts_sample.permute(0, 2, 1)  # [1, 10475, 3]\n",
    "body_verts = body_verts * cube_size  # to local 3d cage coordinate system scale\n",
    "\n",
    "for step in tqdm(range(itr_s2)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    body_params_rec_72 = convert_to_3D_rot(body_params_rec)  # tensor, [bs=1, 72]\n",
    "    body_pose_joint = vposer_model.decode(body_params_rec_72[:, 16:48], output_type='aa').view(1,-1)\n",
    "    body_verts_rec = gen_body_mesh(body_params_rec_72, body_pose_joint, smplx_model)[0]  # [n_body_vert, 3]\n",
    "\n",
    "    # transform body verts to unit ball global coordinate\n",
    "    temp = body_verts_rec / cube_size  # scale into unit ball\n",
    "    body_verts_rec_global = torch.zeros(body_verts_rec.shape).to(device)\n",
    "    body_verts_rec_global[:, 0] = temp[:, 0] * math.cos(rot_angle_2) - temp[:, 1] * math.sin(rot_angle_2)\n",
    "    body_verts_rec_global[:, 1] = temp[:, 0] * math.sin(rot_angle_2) + temp[:, 1] * math.cos(rot_angle_2)\n",
    "    body_verts_rec_global[:, 2] = temp[:, 2]\n",
    "\n",
    "    # calculate body_bps_rec\n",
    "    body_bps_rec = torch.zeros(body_bps_sample.shape)\n",
    "    if weight_loss_rec_bps > 0:\n",
    "        nbrs = NearestNeighbors(n_neighbors=1, leaf_size=1, algorithm=\"ball_tree\").fit(\n",
    "            body_verts_rec_global.detach().cpu().numpy())\n",
    "        neigh_dist, neigh_ind = nbrs.kneighbors(selected_scene_verts_global)\n",
    "        body_bps_rec = body_verts_rec_global[neigh_ind.squeeze()] - \\\n",
    "                       torch.from_numpy(selected_scene_verts_global).float().to(device)  # [n_bps, 3]\n",
    "        body_bps_rec = torch.sqrt(\n",
    "            body_bps_rec[:, 0] ** 2 + body_bps_rec[:, 1] ** 2 + body_bps_rec[:, 2] ** 2).unsqueeze(0).unsqueeze(0)  # [bs=1, 1, n_bps]\n",
    "\n",
    "    ### body bps encoding reconstruct loss\n",
    "    loss_rec_verts = F.l1_loss(body_verts_rec.unsqueeze(0), body_verts)\n",
    "    loss_rec_bps = F.l1_loss(body_bps_sample, body_bps_rec)\n",
    "\n",
    "    ### vposer loss\n",
    "    body_params_rec_72 = convert_to_3D_rot(body_params_rec)\n",
    "    vposer_pose = body_params_rec_72[:, 16:48]\n",
    "    loss_vposer = torch.mean(vposer_pose ** 2)\n",
    "    ### shape prior loss\n",
    "    shape_params = body_params_rec_72[:, 6:16]\n",
    "    loss_shape = torch.mean(shape_params ** 2)\n",
    "    ### hand pose prior loss\n",
    "    hand_params = body_params_rec_72[:, 48:]\n",
    "    loss_hand = torch.mean(hand_params ** 2)\n",
    "\n",
    "    # transfrom body_verts_rec (local 3d cage coordinate system) to prox coordinate system\n",
    "    body_verts_rec_prox = torch.zeros(body_verts_rec.shape).to(device)\n",
    "    temp = body_verts_rec - torch.from_numpy(shift).float().to(device)\n",
    "    body_verts_rec_prox[:, 0] = temp[:, 0] * math.cos(-rot_angle_1) - temp[:, 1] * math.sin(-rot_angle_1)\n",
    "    body_verts_rec_prox[:, 1] = temp[:, 0] * math.sin(-rot_angle_1) + temp[:, 1] * math.cos(-rot_angle_1)\n",
    "    body_verts_rec_prox[:, 2] = temp[:, 2]\n",
    "    body_verts_rec_prox = body_verts_rec_prox.unsqueeze(0)  # tensor, [bs=1, 10475, 3]\n",
    "\n",
    "    ### sdf collision loss\n",
    "    norm_verts_batch = (body_verts_rec_prox - s_grid_min_batch) / (s_grid_max_batch - s_grid_min_batch) * 2 - 1\n",
    "    n_verts = norm_verts_batch.shape[1]\n",
    "    body_sdf_batch = F.grid_sample(s_sdf_batch.unsqueeze(1),\n",
    "                                   norm_verts_batch[:, :, [2, 1, 0]].view(-1, n_verts, 1, 1, 3),\n",
    "                                   padding_mode='border')\n",
    "    # if there are no penetrating vertices then set sdf_penetration_loss = 0\n",
    "    if body_sdf_batch.lt(0).sum().item() < 1:\n",
    "        loss_collision = torch.tensor(0.0, dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        loss_collision = body_sdf_batch[body_sdf_batch < 0].abs().mean()\n",
    "\n",
    "    ### contact loss\n",
    "    body_verts_contact = body_verts_rec.unsqueeze(0)[:, vid, :]  # [1,1121,3]\n",
    "    dist_chamfer_contact = ext.chamferDist()\n",
    "    # scene_verts: [bs=1, n_scene_verts, 3]\n",
    "    scene_verts = torch.from_numpy(scene_verts_local).float().to(device).unsqueeze(0)  # [1,50000,3]\n",
    "    contact_dist, _ = dist_chamfer_contact(body_verts_contact.contiguous(),scene_verts.contiguous())\n",
    "    loss_contact = torch.mean(torch.sqrt(contact_dist + 1e-4) / (torch.sqrt(contact_dist + 1e-4) + 1.0))\n",
    "\n",
    "    loss = weight_loss_rec_verts * loss_rec_verts + weight_loss_rec_bps * loss_rec_bps + \\\n",
    "           weight_loss_vposer * loss_vposer + \\\n",
    "           weight_loss_shape * loss_shape + \\\n",
    "           weight_loss_hand * loss_hand + \\\n",
    "           weight_collision * loss_collision + weight_loss_contact * loss_contact\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "print('[INFO] optimization stage 2 finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Visualize the optimized body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smplx params --> body mesh\n",
    "body_params_opt_s2 = convert_to_3D_rot(body_params_rec)  # tensor, [bs=1, 72]\n",
    "body_pose_joint = vposer_model.decode(body_params_opt_s2[:, 16:48], output_type='aa').view(1, -1)\n",
    "body_verts_opt_s2 = gen_body_mesh(body_params_opt_s2, body_pose_joint, smplx_model)[0]\n",
    "body_verts_opt_s2 = body_verts_opt_s2.detach().cpu().numpy()   # [n_body_vert, 3]\n",
    "\n",
    "# transfrom the body verts to the PROX world coordinate system\n",
    "body_verts_opt_prox_s2 = np.zeros(body_verts_opt_s2.shape)  # [10475, 3]\n",
    "temp = body_verts_opt_s2 - shift\n",
    "body_verts_opt_prox_s2[:, 0] = temp[:, 0] * math.cos(-rot_angle_1) - temp[:, 1] * math.sin(-rot_angle_1)\n",
    "body_verts_opt_prox_s2[:, 1] = temp[:, 0] * math.sin(-rot_angle_1) + temp[:, 1] * math.cos(-rot_angle_1)\n",
    "body_verts_opt_prox_s2[:, 2] = temp[:, 2]\n",
    "\n",
    "body_mesh_opt_s2 = o3d.geometry.TriangleMesh()\n",
    "body_mesh_opt_s2.vertices = o3d.utility.Vector3dVector(body_verts_opt_prox_s2)\n",
    "body_mesh_opt_s2.triangles = o3d.utility.Vector3iVector(smplx_model.faces)\n",
    "body_mesh_opt_s2.compute_vertex_normals()\n",
    "\n",
    "# use normal open3d visualization\n",
    "o3d.visualization.draw_geometries([scene_mesh, body_mesh_opt_s2])  \n",
    "\n",
    "#  # use webGL\n",
    "# visualizer = JVisualizer() \n",
    "# # create pointcloud\n",
    "# pcd_scene = o3d.geometry.PointCloud()\n",
    "# pcd_scene.points = scene_mesh.vertices\n",
    "# pcd_scene.colors = scene_mesh.vertex_colors\n",
    "\n",
    "# pcd_body = o3d.geometry.PointCloud()\n",
    "# pcd_body.points = body_mesh_opt_s2.vertices\n",
    "\n",
    "# visualizer.add_geometry(pcd_scene)\n",
    "# visualizer.add_geometry(pcd_body)\n",
    "# visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
